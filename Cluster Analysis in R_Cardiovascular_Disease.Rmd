---
title: "Cluster Analysis with Cardiovascular data"
author: "Eralda Gjika"
date: "November 2022"
output:
  pdf_document: default
  html_notebook: default
  word_document: default
subtitle: "K-means and Hierarchical Clustering"
---


# Clustering techniques

Libraries used are:
```{r}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
```

Below we are going to work with a dataset of patients and their characteristics related with ur target variable which is: Does a patient suffer from cardiovascular disease or not!

### Distance matrix 
We use get_dist(name_of_dataframe) to obtain the distance matrix which we are using as an argument for fviz_dist() to visualize the distance of the observations. Here "gradient" is a vector of colors used for the display of the three main levels low-mid-high.
```{r}
library(readr)
Vascular_data <- read_csv("D:/ALDA 2021/GitHub EGjika/Vascular data.csv")

distance <- get_dist(Vascular_data)
distance
fviz_dist(distance, gradient = list(low = "green", mid = "white", high = "red"))
```
Changing some color highlits.
```{r}
fviz_dist(distance, gradient = list(low = "blue", mid = "white", high = "red"))

```

What we osberve is the cluster of the observations (patients). Some of them are close (purple color) and some are away from each other (orange color).


# K Means 
We can compute k-means in R with the **kmeans** function. Here will group the data into two clusters (centers = 2). The kmeans function also has an **nstart** option that attempts multiple initial configurations and reports on the best one. For example, adding nstart = 25 will generate 25 initial configurations. This approach is often recommended.
Here only numeric variables are used and three clusters are created.
```{r}
kmean.vasc<-kmeans(Vascular_data[,c(2,6,7,15)], centers = 3, nstart = 25)
kmean.vasc
str(kmean.vasc)
```

We can also view our results by using **fviz_cluster**. This provides a nice illustration of the clusters. **If there are more than two dimensions (variables) fviz_cluster will perform principal component analysis (PCA)** and plot the data points according to the first two principal components that explain the majority of the variance.

```{r}
library(factoextra)
fviz_cluster(kmean.vasc, data=Vascular_data[,c(2,6,7,15)])
```
We may observe that among teh observations the most are located in cluster 2 and less in the other two clusters. But also an overlapping of clusters which is not really helpful. The first two dimessions of PCA together explaing about 70% of the variance.(which seem a good value)

Because the number of clusters (k) must be set before we start the algorithm, it is often advantageous to use several different values of k and examine the differences in the results. We can execute the same process for 3, 4, and 5 clusters, and the results are shown in the figure
```{r}
k2.vasc <- kmeans(Vascular_data[,c(2,6,7,15)], centers = 2, nstart = 25)
k3.vasc <- kmeans(Vascular_data[,c(2,6,7,15)], centers = 3, nstart = 25)
k4.vasc <- kmeans(Vascular_data[,c(2,6,7,15)], centers = 4, nstart = 25)
k5.vasc <- kmeans(Vascular_data[,c(2,6,7,15)], centers = 5, nstart = 25)

p1 <- fviz_cluster(k2.vasc, geom = "point", data = Vascular_data[,c(2,6,7,15)]) + ggtitle("k = 2")
p2 <- fviz_cluster(k3.vasc, geom = "point",  data = Vascular_data[,c(2,6,7,15)]) + ggtitle("k = 3")
p3 <- fviz_cluster(k4.vasc, geom = "point",  data = Vascular_data[,c(2,6,7,15)]) + ggtitle("k = 4")
p4 <- fviz_cluster(k5.vasc, geom = "point",  data =Vascular_data[,c(2,6,7,15)]) + ggtitle("k = 5")

library(gridExtra)# create  a grid where 4 graphs will display
grid.arrange(p1, p2, p3, p4, nrow = 2)
```
Observing the cluster visualization it seems that two clusters is better than more. And based on the values of our target variable (diagnosed or not as a cardiovascular risk patient) then this organization in two clusters makes sense.

## Determine the number of clusters
we may also do some statistical test about the optimal number of clusters.
```{r}
set.seed(123)
fviz_nbclust(Vascular_data[,c(2,6,7,15)], kmeans, method = "wss")
```

## Silouhete method
In short, the average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering.
```{r}
fviz_nbclust(Vascular_data[,c(2,6,7,15)], kmeans, method = "silhouette")
```

### Gap Statistic Method

```{r}
# compute gap statistic
set.seed(123)
gap_stat <- clusGap(Vascular_data[,c(2,6,7,15)], FUN = kmeans, nstart = 25, K.max = 10, B = 50)
# Print the result
print(gap_stat, method = "firstmax")
fviz_gap_stat(gap_stat)
```
The three methods suggest a number of clusters 2, 4 or 6 as the optimal number. So, at this point we may advise to look closer at the patients and also try to understand the behaviour using other cluster methodologies .



# Hierarchial clustering 
```{r}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms
```

Hierarchical clustering-Dendogram
```{r}
# Dissimilarity matrix
d <- dist(Vascular_data[,c(2,6,7,15)], method = "euclidean")

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
```

### Agglometarive -AGNES
Alternatively, we can use the agnes function. These functions behave very similarly; however, with the agnes function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).
```{r}
# Compute with agnes
hc2 <- agnes(Vascular_data[,c(2,6,7,15)], method = "complete")

# Agglomerative coefficient
hc2$ac
```


### Divisive Hierarchical Clustering - DIANA
The R function diana provided by the cluster package allows us to perform divisive hierarchical clustering. DIANA works similar to AGNES; however, there is no method to provide.

```{r}
# compute divisive hierarchical clustering
hc4 <- diana(Vascular_data[,c(2,6,7,15)])

# Divise coefficient; amount of clustering structure found
hc4$dc


# plot dendogram
pltree(hc4, cex = 0.6, hang = -1, main = "Dendogram of diana")
```

The height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.

The height of the cut to the dendogram controls the number of clusters obtained. It plays the same role as the k in k-means clustering. In order to identify sub-groups (i.e. clusters), we can cut the dendrogram with cutree:

```{r}
d <- dist(Vascular_data[,c(2,6,7,15)], method = "euclidean")

# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
sub_grp <- cutree(hc5, k = 4)

# Number of members in each cluster
table(sub_grp)
```
It’s also possible to draw the dendrogram with a border around the 4 clusters. The argument border is used to specify the border colors for the rectangles:

```{r}
plot(hc5, cex = 0.6)
rect.hclust(hc5, k = 5, border = 2:5)# try to change the number of k=3 or 4 and observe
```

# Heatmaps 
May be also used to understand how observations may be organized in clusters.
```{r}
library(pheatmap)
heatmap(as.matrix(Vascular_data[,c(2,6,7,15)]))# the object should be a numeric matrix
```
We observe that BMI is affecting observations differently (it appears in one cluster). The heatmap does clusters for observations and also for variables.


# Bioconductor 
There are many packages used from Bioconductor in R. The code below enables the activation of these packages in R and use them. (just use it once when installing the library)
Reference: https://www.bioconductor.org/

```{r}
# Run this par if you are using this library for the first time
# if (!require("BiocManager", quietly = TRUE))
#  install.packages("BiocManager")
# 
#  BiocManager::install("ComplexHeatmap")
```

```{r}
library(ComplexHeatmap)
Heatmap(as.matrix(Vascular_data[,c(2,6,7,15)]),row_split = ,cluster_columns = ,border=T,heatmap_legend_param = list(title = ""), cluster_row_slices = F)
```

# root clusters
We will use hc5 (the cluster created above form the dendogram). Let's start with 5 and we may observe how many cluster are significant. 
```{r}
library(ape)
# Unrooted
colors = c("red", "blue", "green","yellow","orange")
clus5 = cutree(hc5, 2)# try and change the number of clusters to get an idea based on visualization of the classification tree obtained
plot(as.phylo(hc5), type = "unrooted",  tip.color = colors[clus5],cex = 1, no.margin = TRUE)
```

```{r}
# Cut the dendrogram into 3 clusters
colors = c("red", "blue", "green","yellow","orange")
clus5 = cutree(hc5, 3)
plot(as.phylo(hc5), type = "fan", tip.color = colors[clus5],    label.offset = 1, cex = 0.7)
```



# APPENDIX 1

Lastly, we can also compare two dendograms. Here we compare hierarchical clustering with complete linkage versus Ward’s method. The function tanglegram plots two dendrograms, side by side, with their labels connected by lines.
```{r}
library(dendextend)
# Compute distance matrix
res.dist <- dist(USArrests, method = "euclidean")

# Compute 2 hierarchical clusterings
hc1 <- hclust(res.dist, method = "complete")
hc2 <- hclust(res.dist, method = "ward.D2")

# Create two dendograms
dend1 <- as.dendrogram (hc1)
dend2 <- as.dendrogram (hc2)

tanglegram(dend1, dend2)
```

The output displays “unique” nodes, with a combination of labels/items not present in the other tree, highlighted with dashed lines. The quality of the alignment of the two trees can be measured using the function entanglement. Entanglement is a measure between 1 (full entanglement) and 0 (no entanglement). A lower entanglement coefficient corresponds to a good alignment. The output of tanglegram can be customized using many other options as follow:

```{r}
dend_list <- dendlist(dend1, dend2)

tanglegram(dend1, dend2,
  highlight_distinct_edges = FALSE, # Turn-off dashed lines
  common_subtrees_color_lines = FALSE, # Turn-off line colors
  common_subtrees_color_branches = TRUE, # Color common branches 
  main = paste("entanglement =", round(entanglement(dend_list), 2))
  )
```


